# Suphx：掌握麻将与深度强化学习

论文地址：  <https://arxiv.org/pdf/2003.13590.pdf>

- <https://blog.csdn.net/u013169673/article/details/105486469/>
- <https://blog.csdn.net/qq_42914528/article/details/105383642>

## Suphx AI 算法摘要

### 方法概述

### 决策流程

#### 模型

---

|模型|模型|功能|
|:--|:--|:--|
|discard model|   舍牌模型|通常情况下舍张|
|riichi model|    立直模型| 决定是否立直|
|chow model   |   吃模型|决定是否吃和怎么吃|
|pong model    |  碰模型|决定是否碰|
|kong model     | 杠模型|决定是否杠|

#### 模型结构

日本麻将中总共有34张不同的牌，所以Suphx中使用多个 `34 × 1` 的通道来表示场上的状态，比如手牌可以用四个通道来编码，如下图所示：

![fig3]

类似的，副露、宝牌以及舍牌顺序也通过这种方式进行编码。分类特征（categorical feature）用全为`0`或`1`的多个通道来编码。整数特征（integer feature）则会被区间化，然后每个区间用全为`0`或`1`的通道来编码。

除了这些直接可见的状态之外，作者还设计了look-ahead特征用来表示打出某一张牌后赢牌向听数和分数，例如使用一个特征来表示打出某张牌后能否在进3张牌后和出12000点。由于日麻中总共有89种面子和34种对子，能和的牌型数量非常多，所以作者没有考虑对手的行为（例如通过舍牌判断是否在做混一色/清一色），只是用深度优先搜索来计算自己和出各种牌型的概率。在这种简化之下，总共构建出100多种look-ahead特征，每种特征都用一个34维的向量来表示。

除了输入和输出维度之外，所有模型的网络结构都差不多，具体结构和维度如下图下表所示。在吃、碰、杠模型中，除了状态特征和look-ahead特征以外，还有对哪些牌吃、碰、杠的信息。另外，这些模型都是没有池化层的，因为每个通道中的每一列都有自己的含义，所以池化之后会导致信息损失。

### 算法

Suphx 学习过程分三个阶段

- 通过监督学习训练5个模型。训练用的 (state, action) 对来自天凤平台。
- 用训练过的模型作为策略进行 *self-play*， 并通过*熵正则化分布式强化学习（Distributed Reinforcement Learning with Entropy Regularization）*来更新策略。在训练过程中使用了*全局奖励预测（global reward predictction）*和*先知教练（oracle guiding）*来处理麻将中特有的困难之处。
- 在online playing过程中使用了*运行时策略适应（run-time policy adaption）*来针对本局游戏的初始状态进行调整以获得更好的表现

#### 强化学习

---

self-play

#### 熵正则化分布式强化学习

---

#### 全局预测奖励(Global Reward Predictction)

在一场游戏中有许多局组成，每一局结束后，和牌的选手会获得正分，其他选手则为0分或者负分。

每一场游戏结束后，根据每一局的得分计算最终得分并计算排名 。

然而以每一局的得分或者游戏的最终得分来作为强化学习的训练信号都不太合适，因为：

- 如果用一场游戏的最终得分来训练，则每一局都会有一样的训练信号，那么就无法区分打得好跟打得不好的对局；
- 如果用每一局的得分来训练，则不一定能真实反映选手水平。比如在一场游戏的最后几局，有很大优势的一位选手会打得比较保守，可能会让三位或四位的选手的获得胜利来确保自己可以在这一场游戏结束时处于第一位。那么这几局里面的获得的负的分数并不能说明该选手的策略不好，相反这是一个好策略的体现。

#### 先知教练(Oracle Guiding)

在麻将中有非常多的隐藏信息，在这种情况下用强化学习来学习策略会非常慢。为了加速训练，作者使用了一个可以获取完全信息的先知智能体（oracle agent）。完全信息包括：

1. 当前选手的手牌
2. 所有选手的副露及弃牌
3. 其他公开的信息比如累计积分数、立直棒
4. 另外三名选手的手牌
5. 牌山上的牌

对于一个通常智能体（normal agent，指没有完全信息的agent）来说，只有前三项是可见的。

由于拥有完全信息，oracle agent可以通过强化学习很快精通麻将，问题是怎么让oracle agent来加速normal agent的训练。在这里普通的知识蒸馏（knowledge distillation）的效果并不好，因为一个没有完全信息的normal agent很难去模仿oracle agent的行为。在Suphx中采用的方式是先用完全信息进行训练oracle agent，然后通过drop out的方式逐渐减少完全信息中的特征，慢慢地让oracle agent转变为normal agent。在oracle agent完全转变为normal agent后，还会进行一定轮次的训练。此时学习率会降到之前的1 / 10，并且会拒绝重要性大于某个阈值的state-action对。如果不添加这两个限制，后续的训练就会不稳定，性能也不会获得提升。

#### 参数化蒙特卡洛策略适应（Parametric Monte-Carlo Policy Adaption）

在麻将中，人类会在拿到不同的手牌的时候使用不同的策略，比如会在拿到好的起手的时候打得更激进来赢得更多分数，在拿到不太好的起手的时候打得保守一点来避免更大的损失。所以如果可以把离线训练的策略针对起手做一定的适应，那么就很可能可以获得更好的表现。跟围棋或星际争霸不同，蒙特卡洛树搜索（Monte-Carlo tree search，MCTS）在麻将上的表现并不好，因此作者提出了一种新的方法，名为参数化蒙特卡洛策略适应（parametric Monte-Carlo policy adaption，pMCPA）。

当一局麻将开始，`agent`摸了初始手牌之后，对离线训练的策略按以下方式进行调整：

1. 模拟：固定自己的手牌，对另外三个选手的手牌及牌山的牌进行随机采样，然后用离线训练的策略来试运行游戏并记录出牌顺序。总共记录K个出牌顺序。
2. 适应：根据这K个出牌顺序用策略梯度来微调离线训练的策略。
3. 推理：用微调过的策略来进行本局游戏。

![fig1]
![fig2]
[fig1]:<http://test.com>
[fig2]:<http://test.com>
[fig3]:<https://pic-1257414393.cos.ap-hongkong.myqcloud.com/tenpai_project/suphx_figure_3.png>
