# Suphx：掌握麻将与深度强化学习

论文地址：  <https://arxiv.org/pdf/2003.13590.pdf>

## Suphx AI 算法摘要

### 方法概述

### 模型

|模型|模型|功能|
|:--|:--|:--|
|discard model|   舍牌模型|通常情况下舍张|
|riichi model|    立直模型| 决定是否立直|
|chow model   |   吃模型|决定是否吃和怎么吃|
|pong model    |  碰模型|决定是否碰|
|kong model     | 杠模型|决定是否杠|

---

### 算法

Suphx 学习过程分三个阶段

- 通过监督学习训练5个模型。训练用的 (state, action) 对来自天凤平台。
- 用训练过的模型作为策略进行 *self-play*， 并通过*熵正则化分布式强化学习（Distributed Reinforcement Learning with Entropy Regularization）*来更新策略。在训练过程中使用了*全局奖励预测（global reward predictction）*和*先知教练（oracle guiding）*来处理麻将中特有的困难之处。
- 在online playing过程中使用了*运行时策略适应（run-time policy adaption）*来针对本局游戏的初始状态进行调整以获得更好的表现

### 强化学习

---

self-play

### 熵正则化分布式强化学习

---

### 全局预测奖励(Global Reward Predictction)

在一场游戏中有许多局组成，每一局结束后，和牌的选手会获得正分，其他选手则为0分或者负分。

每一场游戏结束后，根据每一局的得分计算最终得分并计算排名 。

然而以每一局的得分或者游戏的最终得分来作为强化学习的训练信号都不太合适，因为：

- 如果用一场游戏的最终得分来训练，则每一局都会有一样的训练信号，那么就无法区分打得好跟打得不好的对局；
- 如果用每一局的得分来训练，则不一定能真实反映选手水平。比如在一场游戏的最后几局，有很大优势的一位选手会打得比较保守，可能会让三位或四位的选手的获得胜利来确保自己可以在这一场游戏结束时处于第一位。那么这几局里面的获得的负的分数并不能说明该选手的策略不好，相反这是一个好策略的体现。

### 先知教练(Oracle Guiding)

在麻将中有非常多的隐藏信息，在这种情况下用强化学习来学习策略会非常慢。为了加速训练，作者使用了一个可以获取完全信息的先知智能体（oracle agent）。完全信息包括：

1. 当前选手的手牌
2. 所有选手的副露及弃牌
3. 其他公开的信息比如累计积分数、立直棒
4. 另外三名选手的手牌
5. 牌山上的牌

对于一个通常智能体（normal agent，指没有完全信息的agent）来说，只有前三项是可见的。

由于拥有完全信息，oracle agent可以通过强化学习很快精通麻将，问题是怎么让oracle agent来加速normal agent的训练。在这里普通的知识蒸馏（knowledge distillation）的效果并不好，因为一个没有完全信息的normal agent很难去模仿oracle agent的行为。在Suphx中采用的方式是先用完全信息进行训练oracle agent，然后通过drop out的方式逐渐减少完全信息中的特征，慢慢地让oracle agent转变为normal agent。在oracle agent完全转变为normal agent后，还会进行一定轮次的训练。此时学习率会降到之前的1 / 10，并且会拒绝重要性大于某个阈值的state-action对。如果不添加这两个限制，后续的训练就会不稳定，性能也不会获得提升。

### 参数化蒙特卡洛策略适应（Parametric Monte-Carlo Policy Adaption）

在麻将中，人类会在拿到不同的手牌的时候使用不同的策略，比如会在拿到好的起手的时候打得更激进来赢得更多分数，在拿到不太好的起手的时候打得保守一点来避免更大的损失。所以如果可以把离线训练的策略针对起手做一定的适应，那么就很可能可以获得更好的表现。跟围棋或星际争霸不同，蒙特卡洛树搜索（Monte-Carlo tree search，MCTS）在麻将上的表现并不好，因此作者提出了一种新的方法，名为参数化蒙特卡洛策略适应（parametric Monte-Carlo policy adaption，pMCPA）。

当一局麻将开始，agent摸了初始手牌之后，对离线训练的策略按以下方式进行调整：

1. 模拟：固定自己的手牌，对另外三个选手的手牌及牌山的牌进行随机采样，然后用离线训练的策略来试运行游戏并记录出牌顺序。总共记录K个出牌顺序。
2. 适应：根据这K个出牌顺序用策略梯度来微调离线训练的策略。
3. 推理：用微调过的策略来进行本局游戏。
